	While writing the simulation, I just used constant values without reading from a config file, just to keep the values consistent while forcing the simulation to run. Ironically, during testing, using a time-based seed ensured that if there were bugs, they would turn up while runnig. Once the simulation was complete, I needed to troubleshoot specific bugs, and by isolating them with the seed, I was easily able to identify the issue.
	In choosing the parameters, it made sense to choose cpu and disk times as the same just for consistency, but eventually, i settled with disk times being longer than the cpu time, as that's more consistent with a real disk. The most crutial parameter was the arrival time for new jobs. Without editing simulation time, the arrival time determined with a very high sensitivity how many events would occur. I was aiming for more than a thousand events to be processed, and any more than 10,000 was too slow for my computer. By decreasing the arrival time by small amounts, the number of incoming events would jump exponentially. At one point, I thought there was a critical error with my simulation, but I later attributed it to massive amounts of events to be processed. After finding a range of arrival values that worked in relation to the other parameters, my final statistics got a lot more consistent. Then I was able to manipulate other parameters and expirement freely.
